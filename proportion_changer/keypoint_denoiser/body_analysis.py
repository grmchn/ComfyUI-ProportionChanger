"""
Body Analysis and Orientation Detection
ä½“çµ±è¨ˆåˆ†æãƒ»æ­£é¢åº¦æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
try:
    from .config import DenoiserConfig
except ImportError:
    # ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³ãƒ†ã‚¹ãƒˆç”¨
    from config import DenoiserConfig

def distance(p1: np.ndarray, p2: np.ndarray) -> float:
    """2ç‚¹é–“ã®è·é›¢ã‚’è¨ˆç®—"""
    return np.linalg.norm(np.array(p1) - np.array(p2))

def extract_pose_keypoints_from_frame(frame_people_data: Dict) -> List[List[float]]:
    """
    ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰pose_keypointsã‚’æŠ½å‡º
    [x1, y1, c1, x2, y2, c2, ...] -> [[x1, y1, c1], [x2, y2, c2], ...]
    """
    pose_keypoints = frame_people_data.get('pose_keypoints_2d', [])
    
    keypoints = []
    for i in range(0, len(pose_keypoints), 3):
        if i + 2 < len(pose_keypoints):
            keypoints.append([
                pose_keypoints[i],      # x
                pose_keypoints[i + 1],  # y  
                pose_keypoints[i + 2]   # confidence
            ])
    
    # 25å€‹ã®ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã«æº€ãŸãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
    while len(keypoints) < 25:
        keypoints.append([0.0, 0.0, 0.0])
    
    return keypoints[:25]  # 25å€‹ã«åˆ¶é™

def calculate_enhanced_orientation_score(pose_data: List[List[float]], verbose: bool = False) -> float:
    """
    æ”¹è‰¯ã•ã‚ŒãŸæ­£é¢åº¦è¨ˆç®—ï¼šè‚©å¹…ãƒ»è…°å¹…æ­£è¦åŒ– + é¡”è£œå¼·
    0.0=å´é¢, 1.0=æ­£é¢
    """
    if len(pose_data) < 25:
        if verbose:
            print("      âš ï¸ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ•°ä¸è¶³ï¼ˆ<25ï¼‰")
        return 0.5
    
    # è‚©å¹…ãƒ»è…°å¹…å–å¾—
    shoulder_w = hip_w = 0
    confidence_threshold = 0.3
    
    if (pose_data[2][2] > confidence_threshold and pose_data[5][2] > confidence_threshold):  # ä¸¡è‚©è¦‹ãˆã‚‹
        shoulder_w = distance(pose_data[2][:2], pose_data[5][:2])
    if (pose_data[8][2] > confidence_threshold and pose_data[11][2] > confidence_threshold):  # ä¸¡è…°è¦‹ãˆã‚‹
        hip_w = distance(pose_data[8][:2], pose_data[11][:2])
    
    # ä»®ã®æ­£é¢åŸºæº–ï¼ˆçµ±è¨ˆçš„åŸºæº–ã§å¾Œã§ç½®ãæ›ãˆäºˆå®šï¼‰
    w_sh_est = 0.12  # ä»®ã®æ­£é¢æ™‚è‚©å¹…
    w_hp_est = 0.10  # ä»®ã®æ­£é¢æ™‚è…°å¹…
    
    # åŸºæœ¬æ­£é¢åº¦ï¼ˆè‚©å¹…ãƒ»è…°å¹…ã®æ­£è¦åŒ–å¹³å‡ï¼‰
    ori_components = []
    if shoulder_w > 0.02:  # æœ€å°é–¾å€¤
        ori_components.append(min(shoulder_w / w_sh_est, 1.0))
    if hip_w > 0.02:
        ori_components.append(min(hip_w / w_hp_est, 1.0))
    
    if ori_components:
        base_orientation = np.mean(ori_components)
    else:
        base_orientation = 0.5
    
    # é¡”è£œå¼·ï¼ˆç›®ãƒ»å£ã®æ¨ªå¹…/ç¸¦å¹… ã§é¡”yawä»£ç†ï¼‰
    face_bonus = 0
    if (pose_data[14][2] > confidence_threshold and pose_data[15][2] > confidence_threshold):  # ä¸¡ç›®
        eye_distance = distance(pose_data[14][:2], pose_data[15][:2])
        if eye_distance > 0.015:  # æ­£è¦åŒ–åº§æ¨™ã§é©åº¦ãªç›®é–“è·é›¢
            face_bonus += 0.1
    
    # æœ€çµ‚æ­£é¢åº¦ã‚¹ã‚³ã‚¢
    final_score = np.clip(base_orientation + face_bonus, 0, 1)
    
    if verbose:
        print(f"      ğŸ“ è‚©å¹…={shoulder_w:.4f}, è…°å¹…={hip_w:.4f}")
        print(f"      ğŸ“ åŸºæœ¬æ­£é¢åº¦={base_orientation:.3f}, é¡”è£œå¼·={face_bonus:.3f}")
        print(f"      ğŸ“ æœ€çµ‚æ­£é¢åº¦ã‚¹ã‚³ã‚¢={final_score:.3f}")
    
    return final_score

def calculate_joint_distances_enhanced(pose_data: List[List[float]], verbose: bool = False) -> Dict[str, float]:
    """
    å„é–¢ç¯€é–“è·é›¢ã‚’è¨ˆç®—ï¼ˆä¿¡é ¼åº¦ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    """
    distances = {}
    
    # ä¿¡é ¼åº¦é–¾å€¤
    confidence_threshold = 0.3
    
    # è‚©å¹…ï¼ˆæ­£é¢æ™‚ã®ã¿æœ‰åŠ¹ï¼‰
    if (pose_data[2][2] > confidence_threshold and pose_data[5][2] > confidence_threshold):
        shoulder_width = distance(pose_data[2][:2], pose_data[5][:2])
        if shoulder_width > 0.02:  # æœ€å°é–¾å€¤ï¼ˆå´é¢æ™‚ã¯é™¤å¤–ï¼‰
            distances['shoulder_width'] = shoulder_width
    
    # è…°å¹…
    if (pose_data[8][2] > confidence_threshold and pose_data[11][2] > confidence_threshold):
        hip_width = distance(pose_data[8][:2], pose_data[11][:2])
        if hip_width > 0.02:
            distances['hip_width'] = hip_width
    
    # èƒ´ä½“é•·ï¼ˆé¦–-è…°ä¸­ç‚¹ï¼‰
    if (pose_data[1][2] > confidence_threshold and 
        pose_data[8][2] > confidence_threshold and pose_data[11][2] > confidence_threshold):
        hip_center = ((pose_data[8][0] + pose_data[11][0]) / 2, 
                      (pose_data[8][1] + pose_data[11][1]) / 2)
        distances['torso_length'] = distance(pose_data[1][:2], hip_center)
    
    # é ­éƒ¨ã‚µã‚¤ã‚ºï¼ˆé¼»-é¦–ï¼‰
    if (pose_data[0][2] > confidence_threshold and pose_data[1][2] > confidence_threshold):
        distances['head_size'] = distance(pose_data[0][:2], pose_data[1][:2])
    
    # ä¸Šè…•ãƒ»å‰è…•ãƒ»å¤ªè…¿ãƒ»ä¸‹è…¿ï¼ˆå·¦å³ï¼‰
    joint_pairs = {
        'upper_arm_r': (2, 3),   # å³è‚©-å³è‚˜
        'upper_arm_l': (5, 6),   # å·¦è‚©-å·¦è‚˜
        'forearm_r': (3, 4),     # å³è‚˜-å³æ‰‹é¦–
        'forearm_l': (6, 7),     # å·¦è‚˜-å·¦æ‰‹é¦–
        'thigh_r': (11, 12),     # å³è…°-å³è†
        'thigh_l': (8, 9),       # å·¦è…°-å·¦è†
        'calf_r': (12, 13),      # å³è†-å³è¶³é¦–
        'calf_l': (9, 10),       # å·¦è†-å·¦è¶³é¦–
    }
    
    for joint_name, (idx1, idx2) in joint_pairs.items():
        if (pose_data[idx1][2] > confidence_threshold and 
            pose_data[idx2][2] > confidence_threshold):
            distances[joint_name] = distance(pose_data[idx1][:2], pose_data[idx2][:2])
    
    if verbose:
        print(f"      ğŸ“ æ¤œå‡ºé–¢ç¯€æ•°: {len(distances)}")
        for joint_name, dist in distances.items():
            print(f"      ğŸ“   {joint_name}: {dist:.4f}")
    
    return distances

def analyze_body_statistics_enhanced(pose_keypoint_batch: List, config: DenoiserConfig) -> Dict:
    """
    æ”¹è‰¯ç‰ˆï¼šæ­£é¢åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‹é ‘å¥çµ±è¨ˆã«ã‚ˆã‚‹ä½“ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»éª¨é•·åŸºæº–è¨ˆç®—
    é‡è¦æ”¹å–„ç‚¹ï¼šã€Œæ­£é¢å‘ããƒ•ãƒ¬ãƒ¼ãƒ é›†åˆã€ã§éª¨é•·åŸºæº–ã‚’å­¦ç¿’ã—ã€å´é¢ãƒã‚¤ã‚¢ã‚¹ã‚’å›é¿
    """
    
    if config.verbose_logging:
        print("  ğŸ” [Phase 1-2] ä½“çµ±è¨ˆåˆ†æé–‹å§‹...")
        print(f"    ğŸ“Š ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(pose_keypoint_batch)}")
    
    # å„éƒ¨ä½ã®è·é›¢çµ±è¨ˆã‚’åé›†ï¼ˆæ­£é¢ãƒ•ãƒ¬ãƒ¼ãƒ å„ªå…ˆï¼‰
    joint_distance_stats = {
        'shoulder_width': [],      # è‚©å¹…ï¼ˆæ­£é¢æ™‚ã®ã¿æœ‰åŠ¹ï¼‰
        'hip_width': [],           # è…°å¹…ï¼ˆæ­£é¢åº¦åˆ¤å®šè£œå¼·ç”¨ï¼‰
        'torso_length': [],        # èƒ´ä½“é•·ï¼ˆé¦–-è…°ä¸­ç‚¹ï¼‰
        'upper_arm_r': [], 'upper_arm_l': [],   # ä¸Šè…•ï¼ˆè‚©-è‚˜ï¼‰
        'forearm_r': [], 'forearm_l': [],       # å‰è…•ï¼ˆè‚˜-æ‰‹é¦–ï¼‰
        'thigh_r': [], 'thigh_l': [],           # å¤ªè…¿ï¼ˆè…°-è†ï¼‰
        'calf_r': [], 'calf_l': [],             # ä¸‹è…¿ï¼ˆè†-è¶³é¦–ï¼‰
        'head_size': [],           # é ­éƒ¨ã‚µã‚¤ã‚ºï¼ˆé¼»-é¦–ï¼‰
    }
    
    # ä½“ã®å‘ãåˆ†æï¼ˆæ­£é¢åº¦åˆ¤å®šç”¨ï¼‰
    orientation_scores = []
    front_facing_frames = []  # é«˜æ­£é¢åº¦ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    
    if config.verbose_logging:
        print("    ğŸ“ Phase 1-2a: å…¨ãƒ•ãƒ¬ãƒ¼ãƒ æ­£é¢åº¦åˆ†æ...")
    
    # Phase 1: å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã§æ­£é¢åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
    for frame_idx, frame_data in enumerate(pose_keypoint_batch):
        if not frame_data or not frame_data[0].get('people'):
            continue
            
        pose_data = extract_pose_keypoints_from_frame(frame_data[0]['people'][0])
        
        # æ”¹è‰¯ã•ã‚ŒãŸæ­£é¢åº¦è¨ˆç®—ï¼ˆè‚©å¹…ãƒ»è…°å¹…ãƒ»ç›®é–“è·é›¢ãƒ»é¡”yawä»£ç†ï¼‰
        orientation_score = calculate_enhanced_orientation_score(
            pose_data, verbose=config.verbose_logging and frame_idx < 3  # æœ€åˆã®3ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿è©³ç´°ãƒ­ã‚°
        )
        orientation_scores.append(orientation_score)
        
        # é«˜æ­£é¢åº¦ãƒ•ãƒ¬ãƒ¼ãƒ é¸å®šï¼ˆé¡”yawâ‰ˆ0, ç›®é–“è·é›¢ãŒé«˜ä½ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã€confé«˜ï¼‰
        if orientation_score > config.orientation_threshold:
            front_facing_frames.append(frame_idx)
    
    if config.verbose_logging:
        avg_orientation = np.mean(orientation_scores) if orientation_scores else 0.5
        print(f"    ğŸ“ å¹³å‡æ­£é¢åº¦: {avg_orientation:.3f}")
        print(f"    ğŸ“ é«˜æ­£é¢åº¦ãƒ•ãƒ¬ãƒ¼ãƒ : {len(front_facing_frames)}å€‹ (é–¾å€¤>{config.orientation_threshold})")
    
    if config.verbose_logging:
        print("    ğŸ“ Phase 1-2b: éª¨é•·çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åé›†...")
    
    # Phase 2: æ­£é¢ãƒ•ãƒ¬ãƒ¼ãƒ ç¾¤ã§éª¨é•·åŸºæº–å­¦ç¿’
    for frame_idx, frame_data in enumerate(pose_keypoint_batch):
        if not frame_data or not frame_data[0].get('people'):
            continue
            
        pose_data = extract_pose_keypoints_from_frame(frame_data[0]['people'][0])
        distances = calculate_joint_distances_enhanced(
            pose_data, verbose=config.verbose_logging and frame_idx == 0  # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿è©³ç´°ãƒ­ã‚°
        )
        
        # æ­£é¢ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆï¼šå…¨é–¢ç¯€ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        if frame_idx in front_facing_frames:
            for joint_name, distance_value in distances.items():
                if distance_value > 0 and joint_name in joint_distance_stats:
                    joint_distance_stats[joint_name].append(distance_value)
        else:
            # å´é¢ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆï¼šå‘ãã«ä¾å­˜ã—ãªã„é–¢ç¯€ã®ã¿åé›†
            orientation_independent = ['torso_length', 'head_size']
            for joint_name in orientation_independent:
                if joint_name in distances and distances[joint_name] > 0:
                    joint_distance_stats[joint_name].append(distances[joint_name])
    
    if config.verbose_logging:
        print("    ğŸ§® Phase 1-2c: é ‘å¥çµ±è¨ˆå‡¦ç†ï¼ˆTukeyãƒ•ã‚§ãƒ³ã‚¹ï¼‰...")
    
    # Phase 3: é ‘å¥çµ±è¨ˆã«ã‚ˆã‚‹åŸºæº–å€¤è¨ˆç®—ï¼ˆTukeyã®ãƒ•ã‚§ãƒ³ã‚¹ or MADä½¿ç”¨ï¼‰
    body_references = {}
    
    for joint_name, distances in joint_distance_stats.items():
        if len(distances) >= config.min_samples_for_stats:
            distances_array = np.array(distances)
            
            # Tukeyã®ãƒ•ã‚§ãƒ³ã‚¹ã«ã‚ˆã‚‹å¤–ã‚Œå€¤é™¤å»ï¼ˆæ¨å¥¨æ‰‹æ³•ï¼‰
            Q1, Q3 = np.percentile(distances_array, [25, 75])
            IQR = Q3 - Q1
            lower_fence = Q1 - config.outlier_fence_multiplier * IQR
            upper_fence = Q3 + config.outlier_fence_multiplier * IQR
            
            # ãƒ•ã‚§ãƒ³ã‚¹å†…ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
            clean_distances = distances_array[
                (distances_array >= lower_fence) & (distances_array <= upper_fence)
            ]
            
            if len(clean_distances) > 0:
                body_references[joint_name] = {
                    'median': np.median(clean_distances),
                    'q25': np.percentile(clean_distances, 25),
                    'q75': np.percentile(clean_distances, 75),
                    'mad': np.median(np.abs(clean_distances - np.median(clean_distances))),  # MADè¿½åŠ 
                    'sample_count': len(clean_distances),
                    'outliers_removed': len(distances) - len(clean_distances)
                }
                
                if config.verbose_logging:
                    ref = body_references[joint_name]
                    print(f"      ğŸ“Š {joint_name}: median={ref['median']:.4f}, samples={ref['sample_count']}, outliers={ref['outliers_removed']}")
    
    if config.verbose_logging:
        print("    ğŸ’ª Phase 1-2d: ä½“ã‚¹ã‚±ãƒ¼ãƒ«æ±ºå®š...")
    
    # Phase 4: ä½“ã‚¹ã‚±ãƒ¼ãƒ«æ±ºå®šï¼ˆè¤‡æ•°éƒ¨ä½ã‹ã‚‰ãƒŸãƒƒã‚¯ã‚¹æ¨å®šï¼‰
    primary_scale_candidates = []
    
    # èƒ´ä½“ãƒ»è‚©å¹…ãƒ»è…°å¹…ãƒ»ä¸Šè…•ã®ä¸­å¤®å€¤ãƒŸãƒƒã‚¯ã‚¹ã§æ¨å®š
    scale_sources = {
        'torso_length': 1.0,        # ãã®ã¾ã¾ä½¿ç”¨
        'shoulder_width': 2.5,      # ä½“å¹…æ›ç®—
        'hip_width': 3.0,           # ä½“å¹…æ›ç®—ï¼ˆã‚„ã‚„ä¿å®ˆçš„ï¼‰
        'upper_arm_r': 6.0,         # ä¸Šè…•é•·ã‹ã‚‰å…¨èº«æ¨å®š
        'upper_arm_l': 6.0,         # ä¸Šè…•é•·ã‹ã‚‰å…¨èº«æ¨å®š
    }
    
    for joint_name, scale_factor in scale_sources.items():
        if joint_name in body_references:
            estimated_scale = body_references[joint_name]['median'] * scale_factor
            primary_scale_candidates.append(estimated_scale)
            
            if config.verbose_logging:
                print(f"      ğŸ’ª {joint_name} -> ä½“ã‚¹ã‚±ãƒ¼ãƒ«æ¨å®š: {estimated_scale:.4f}")
    
    # é ‘å¥ãªä½“ã‚¹ã‚±ãƒ¼ãƒ«æ±ºå®šï¼ˆä¸­å¤®å€¤ä½¿ç”¨ï¼‰
    if primary_scale_candidates:
        primary_body_scale = np.median(primary_scale_candidates)
    else:
        primary_body_scale = config.fallback_body_scale
        if config.verbose_logging:
            print(f"      âš ï¸ ä½“ã‚¹ã‚±ãƒ¼ãƒ«æ¨å®šå¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ä½¿ç”¨: {primary_body_scale}")
    
    # æ­£é¢å‘ãæ¯”ç‡è¨ˆç®—
    front_facing_ratio = len(front_facing_frames) / max(len(pose_keypoint_batch), 1)
    
    result = {
        'primary_scale': primary_body_scale,
        'joint_references': body_references,
        'orientation_stats': {
            'front_facing_ratio': front_facing_ratio,
            'front_frame_count': len(front_facing_frames),
            'avg_orientation': np.mean(orientation_scores) if orientation_scores else 0.5,
            'front_frame_indices': front_facing_frames
        },
        'scale_candidates': primary_scale_candidates,  # ãƒ‡ãƒãƒƒã‚°ç”¨
        'quality_score': min(front_facing_ratio * 2.0, 1.0)  # ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™
    }
    
    if config.verbose_logging:
        print(f"  âœ… [Phase 1-2] ä½“çµ±è¨ˆåˆ†æå®Œäº†")
        print(f"    ğŸ’ª æœ€çµ‚ä½“ã‚¹ã‚±ãƒ¼ãƒ«: {primary_body_scale:.4f}")
        print(f"    ğŸ¯ æ­£é¢å‘ãæ¯”ç‡: {front_facing_ratio:.2%}")
        print(f"    ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢: {result['quality_score']:.3f}")
        print(f"    ğŸ” æœ‰åŠ¹é–¢ç¯€çµ±è¨ˆ: {len(body_references)}ç¨®é¡")
    
    return result